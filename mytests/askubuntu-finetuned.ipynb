{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im here!\n",
      "Im after that!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('./model3-outputs-gpt2tokenizer-askubuntubody-margintesting')\n",
    "#dataset = load_from_disk('./model3-outputs-gpt2tokenizer-askubuntubody-margintraining-150000train-10000test')\n",
    "from mymodelmargin import MySimilarityModel\n",
    "#mymodel = MySimilarityModel.from_pretrained('./nodotmodel3_weights_2024-08-13--01:38:03alaki')\n",
    "mymodel = MySimilarityModel.from_pretrained('./nodotmodel3_weights_2024-08-13--19:33:49alaki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['similar', 'random', 'input_ids'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = dataset['validation'][:5]\n",
    "collator_input = [dict(zip(mydict.keys(), values)) for values in zip(*[mydict[col] for col in mydict.keys()])]\n",
    "collator_input[0].keys()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from mycollators import DataCollatorForTextSimilarity\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mycollator = DataCollatorForTextSimilarity(tokenizer, 10)\n",
    "outputs = mycollator(collator_input)\n",
    "mymodel.to('cuda')\n",
    "list_of_sub_batches = [sub_batch.to('cuda') for sub_batch in outputs['list_of_sub_batches']]\n",
    "model_outputs = mymodel(list_of_sub_batches, outputs['sign'], outputs['indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar scores are  tensor([0.3667, 0.4063], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "random scores are  tensor([0.3663, 0.3945, 0.3486, 0.3631, 0.3146, 0.3482, 0.3340, 0.4246, 0.4210,\n",
      "        0.3299, 0.4211, 0.3544, 0.2430, 0.3667, 0.2777, 0.4063, 0.3877, 0.3866,\n",
      "        0.3870, 0.3712], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print('similar scores are ', model_outputs['similar_scores'][4])\n",
    "print('random scores are ', model_outputs['random_scores'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im here i is  0  j is  0\n",
      "im here i is  0  j is  1\n",
      "im here i is  0  j is  2\n",
      "im here i is  0  j is  3\n",
      "im here i is  0  j is  4\n",
      "im here i is  0  j is  5\n",
      "im here i is  0  j is  6\n",
      "im here i is  0  j is  7\n",
      "im here i is  0  j is  8\n",
      "im here i is  0  j is  9\n",
      "im here i is  1  j is  0\n",
      "im here i is  1  j is  1\n",
      "im here i is  1  j is  2\n",
      "im here i is  1  j is  3\n",
      "im here i is  1  j is  4\n",
      "im here i is  1  j is  5\n",
      "im here i is  1  j is  6\n",
      "im here i is  1  j is  7\n",
      "im here i is  1  j is  8\n",
      "im here i is  1  j is  9\n",
      "im here i is  2  j is  0\n",
      "im here i is  2  j is  1\n",
      "<built-in method count of str object at 0x7d119bfa3b90>\n"
     ]
    }
   ],
   "source": [
    "from openaimodel import OpenAIModel\n",
    "openAImodel = OpenAIModel()\n",
    "model_outputs = openAImodel(list_of_sub_batches, outputs['sign'], outputs['indices'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lading AskUbuntu Question Dataset!\n",
    "import pandas as pd\n",
    "df = pd.read_csv('texts_raw_fixed.txt', sep='\\t', header=None)\n",
    "df.columns = ['qid', 'title', 'body']\n",
    "similar_train_df = pd.read_csv('train_random.txt', sep='\\t', header=None)\n",
    "similar_train_df.columns = ['qid', 'similar', 'random']\n",
    "similar_test_df = pd.read_csv('test.txt', sep='\\t', header=None)\n",
    "similar_test_df.columns = ['qid', 'similar', 'random', 'bm25-scores']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
