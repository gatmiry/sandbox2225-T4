{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Saving the dataset (4/4 shards): 100%|██████████| 2119719/2119719 [00:21<00:00, 100411.87 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 21990/21990 [00:00<00:00, 580748.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "ds.save_to_disk('./tinystories_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = [1,2,4]\n",
    "ar[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce the sizey of tiny stories so the map function does not take so long!\n",
    "from datasets import DatasetDict\n",
    "import numpy as np\n",
    "num_text = 5000\n",
    "ds = DatasetDict({'train' : ds['train'].select(np.arange(num_text)), 'validation': ds['validation'].select(np.arange(num_text))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model and tokenizer function\n",
    "## Mymodel definition\n",
    "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Config, PreTrainedModel\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "\n",
    "class MyModel(PreTrainedModel):\n",
    "    config_class = GPT2Config\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = GPT2Model(config)\n",
    "        self.second_encoder = GPT2Model(config)\n",
    "        self.decoder = GPT2LMHeadModel(config)\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        encoder_outputs = self.encoder(input_ids)\n",
    "        hidden_embedding = encoder_outputs.last_hidden_state[:,-1,:].unsqueeze(1)\n",
    "        # just to obtain the hidden embeddings\n",
    "        with torch.no_grad():\n",
    "            decoder_hidden_inputs = self.second_encoder(input_ids, output_hidden_states=True).hidden_states[0]\n",
    "        #hidden_embedding_dim = hidden_embedding.shape[2]\n",
    "        updated_input = torch.cat((hidden_embedding, decoder_hidden_inputs), dim=1)\n",
    "        logits = self.decoder(inputs_embeds=updated_input)['logits']\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "        shifted_prediction_scores = logits[:, 1:-1, :]\n",
    "        \n",
    "        labels[attention_mask == 0] = -100 \n",
    "        labels = labels[:, 1:]\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        lm_loss = loss_fct(shifted_prediction_scores.contiguous().view(-1, self.config.vocab_size), labels.contiguous().view(-1))\n",
    "        return {'loss': lm_loss, 'logits':logits[:,1:,:]}\n",
    "    \n",
    "\n",
    "## defining tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained('google-t5/t5-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "context_length = 512\n",
    "def tokenize(element):\n",
    "    #print('element is ', len(element['text']))\n",
    "    #return {'input_ids': []}\n",
    "    #print('len is ', ('</s>'.join(x) for x in element['text']).type)\n",
    "    outputs = tokenizer(\n",
    "        element['text'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    input_batch = []\n",
    "    length_batch = []\n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        num_tokens = sum(token != tokenizer.pad_token_id for token in input_ids) - 1\n",
    "        #print('last id is ', input_ids[-1])\n",
    "        if length <= context_length:\n",
    "            input_batch.append(input_ids)\n",
    "            length_batch.append(num_tokens)\n",
    "    #print('batch length is ', len(input_batch))\n",
    "    return {'input_ids': input_batch, 'length': length_batch}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the model\n",
    "#checkpoint = './model3weights_2024-07-04--16:34:15'\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#tokenizer\n",
    "from mymodelnew2 import MyModel\n",
    "checkpoint = 'posmodel3_weights_2024-07-29--17:41:44alaki'\n",
    "model = MyModel.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "## don't run this unless you want t5 model instead!\n",
    "## initializing T5 encoder model\n",
    "### obtaining the average of the embeddings\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small')\n",
    "model = T5EncoderModel.from_pretrained('google-t5/t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5000/5000 [00:00<00:00, 50705.08 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5000/5000 [00:00<00:00, 38646.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## build the raw dataset\n",
    "tokenized_dataset_raw = ds.map(tokenize, batched=True, remove_columns=ds['train'].column_names)\n",
    "tokenized_dataset_raw.save_to_disk('./tinystories-validation-ctxlen=512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## subsample the raw dataset\n",
    "import numpy as np\n",
    "#num_text = 5000\n",
    "tokenized_dataset = tokenized_dataset_raw['validation'].select(np.arange(num_text))\n",
    "## move model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "model.to(device)\n",
    "#tokenized_dataset = {'input_ids': tokenized_dataset['input_ids'].to(device)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't run this!!\n",
    "## new dataset handling\n",
    "from datasets import load_from_disk\n",
    "tokenized_dataset_raw = load_from_disk('./model3-outputs-gpt2tokenizer')\n",
    "tokenized_dataset = tokenized_dataset_raw['validation'].select(np.arange(num_text))\n",
    "## changing model to cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 120\n",
    "num_batch = int(num_text / batch_size)\n",
    "all_hidden_embeddings = torch.empty(0, model.config.hidden_size).to(device)\n",
    "with torch.no_grad():\n",
    "    for i in range(num_batch):\n",
    "        #print(len(model.encoder(torch.tensor(tokenized_dataset['input_ids'])[i*batch_size:(i+1)*batch_size,:].to(device))))\n",
    "        #print('salam')\n",
    "        hidden_embeddings = model.encoder(torch.tensor(tokenized_dataset['input_ids'])[i*batch_size:(i+1)*batch_size,:].to(device)).last_hidden_state[np.arange(batch_size), tokenized_dataset['length'][i*batch_size:(i+1)*batch_size], :] \n",
    "        all_hidden_embeddings = torch.cat((all_hidden_embeddings, hidden_embeddings), dim=0)\n",
    "all_hidden_embeddings.shape\n",
    "    #torch.cuda.empty_cache()\n",
    "#all_hidden_embeddings = torch.tensor(all_hidden_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "mytensor = torch.tensor([[1,2],[3,4]])\n",
    "mytensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 700, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## don't run this unless you want to find embeddings using T5 encoder!\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = 100\n",
    "num_batch = int(num_text / batch_size)\n",
    "all_hidden_embeddings = torch.empty(0, context_length, model.config.hidden_size).to(device)\n",
    "with torch.no_grad():\n",
    "    for i in range(num_batch):\n",
    "        #print(len(model.encoder(torch.tensor(tokenized_dataset['input_ids'])[i*batch_size:(i+1)*batch_size,:].to(device))))\n",
    "        #print('salam')\n",
    "        hidden_embeddings = model.encoder(torch.tensor(tokenized_dataset['input_ids'])[i*batch_size:(i+1)*batch_size,:].to(device)).last_hidden_state\n",
    "        all_hidden_embeddings = torch.cat((all_hidden_embeddings, hidden_embeddings), dim=0)\n",
    "all_hidden_embeddings.shape\n",
    "    #torch.cuda.empty_cache()\n",
    "#all_hidden_embeddings = torch.tensor(all_hidden_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = model.encoder(torch.tensor([tokenized_dataset['input_ids'][0]]).to('cuda')).last_hidden_state\n",
    "#result[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## run this only if you are finding the embeddings with T5 encoder!\n",
    "\n",
    "## run this only if you are using average token embedding!\n",
    "#alaki = torch.rand((3,4))\n",
    "#condition = torch.unsqueeze(torch.arange(3), dim=0).repeat(2,1).t()\n",
    "#condition\n",
    "\n",
    "valid_len = torch.unsqueeze(torch.tensor(tokenized_dataset['length']), dim=1).repeat(1, context_length)\n",
    "#print('len tokenized dataset is ', len(tokenized_dataset['input_ids']))\n",
    "row_index = torch.unsqueeze(torch.arange(context_length), dim=0).repeat(len(tokenized_dataset['input_ids']), 1)\n",
    "condition = row_index <= valid_len\n",
    "condition = torch.unsqueeze(condition, dim=2).repeat(1,1,model.config.hidden_size)\n",
    "#all_hidden_embeddings.shape\n",
    "#condition.shape\n",
    "all_hidden_embeddings_processed = torch.where(condition.to(device), all_hidden_embeddings, 0.0)\n",
    "all_hidden_embeddings = torch.mean(all_hidden_embeddings_processed, dim=1)\n",
    "all_hidden_embeddings.shape\n",
    "#all_hidden_embeddings[valid_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4920, 768)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_hidden_embeddings = np.array(all_hidden_embeddings.to('cpu')).astype('float32')\n",
    "np_hidden_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "close text index is 771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a little girl named Lily. She liked to play outside in the sunshine. One day, she saw a butterfly. The butterfly was very pretty and had many colors. \\n\\nLily said, \"Hello, butterfly! Can I play with you?\" \\n\\nThe butterfly said, \"Yes, you can play with me. But I can\\'t stay too long. I will melt if I stay in the sun too much.\" \\n\\nLily said, \"Oh no! I don\\'t want you to melt. Let\\'s go find some shade.\" \\n\\nThey found a tree with lots of shade. The butterfly said, \"Thank you for finding a shady spot for me. You are very kind.\" \\n\\nLily smiled and said, \"You\\'re welcome. I like to help insects when they need it. I have some water available if you\\'re thirsty.\" \\n\\nThe butterfly drank the water and then flew away. Lily was happy that she could help the butterfly. She went back to playing outside, feeling proud of herself for being kind to the little insect.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pick one of the stories as the base for searech\n",
    "#tokenized_dataset\n",
    "base_index = 9\n",
    "input_text = tokenizer.decode(tokenized_dataset['input_ids'][base_index], skip_special_tokens=True)\n",
    "input_text\n",
    "import faiss\n",
    "index_flat = faiss.IndexFlatL2(model.config.hidden_size)\n",
    "res = faiss.StandardGpuResources()\n",
    "gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "gpu_index_flat.add(np_hidden_embeddings)\n",
    "k = 2\n",
    "D, I = gpu_index_flat.search(np.expand_dims(np_hidden_embeddings[base_index], axis=0), k)\n",
    "close_text = tokenizer.decode(tokenized_dataset['input_ids'][I[0][1]], skip_special_tokens=True)\n",
    "print('close text index is ' + str(I[0][1]))\n",
    "close_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6828e+00, -1.3115e+00,  6.2090e-01,  1.3430e+00,  2.1449e+00,\n",
      "         1.0749e-01,  2.9974e-01, -1.7864e+00,  9.9046e-01,  1.0543e+00,\n",
      "         2.7673e-01,  1.3266e+00,  8.0879e-01,  1.0688e+00,  4.0232e-01,\n",
      "        -4.5541e-01,  1.4120e+00, -2.3848e-01,  2.9317e+00, -1.2302e+00,\n",
      "         3.4106e-01,  1.5886e+00, -1.9384e-01,  4.9927e-01, -1.7283e+00,\n",
      "         1.1958e-01, -1.1766e+00, -6.0489e-01, -2.6862e-01,  1.3256e+00,\n",
      "        -3.7113e-01,  2.7773e-01,  3.7017e-01,  6.9553e-01,  1.2615e+00,\n",
      "        -2.7678e-01,  7.2701e-01, -7.8779e-01,  9.7299e-01, -1.5636e-01,\n",
      "         6.5418e-01, -9.2435e-01,  1.1597e+00, -1.3186e+00,  8.8128e-01,\n",
      "         2.0889e-01, -9.1116e-01, -1.7272e-01, -3.9783e-01,  2.1831e+00,\n",
      "         1.9208e+00, -9.6646e-01,  1.7172e-01,  1.1205e+00, -4.9456e-01,\n",
      "         3.3881e-01,  1.6079e+00,  7.1630e-01, -2.9640e-01,  6.2130e-01,\n",
      "        -8.5389e-01,  6.8214e-01, -1.2316e-01,  7.9154e-01, -4.7565e-01,\n",
      "        -6.5235e-01,  1.1552e+00, -5.0727e-01,  1.5321e+00, -1.9726e-01,\n",
      "        -7.3921e-01,  1.4438e+00, -1.6318e+00, -1.4648e+00, -2.3536e+00,\n",
      "         9.5803e-01,  3.7077e-02,  1.0117e+00, -1.3080e+00,  1.8383e+00,\n",
      "         1.3947e-01,  2.9420e+00,  1.0845e+00,  4.7624e-01,  4.8046e-01,\n",
      "         1.3898e+00,  5.5659e-01, -1.3697e+00, -3.5285e-02,  7.4094e-01,\n",
      "        -5.2937e-01, -9.7359e-01,  1.6878e+00,  1.6186e+00, -1.7616e+00,\n",
      "        -3.5852e+00, -3.2228e+00, -3.6051e-01,  4.4038e-01,  1.7622e-02,\n",
      "         1.3595e+00,  1.2632e+00, -6.3426e-01, -1.2306e-01, -6.3629e-02,\n",
      "        -1.3435e+00, -9.0731e-01,  1.0159e+00,  6.3300e-01,  1.7226e+00,\n",
      "         1.4839e+00,  2.9229e-01, -1.7406e+00, -8.8482e-01,  3.3192e-01,\n",
      "         2.5693e-01,  1.0792e+00, -2.1834e+00, -3.1047e-01,  3.7199e-01,\n",
      "        -7.6946e-01, -1.3351e-01,  2.2637e-02,  1.3616e-01,  3.6793e-01,\n",
      "        -2.0732e+00,  9.0209e-01,  1.5240e+00,  2.1505e-01, -6.0711e-01,\n",
      "         3.5106e-01, -5.9776e-01, -1.1619e+00,  1.0272e+00, -9.0803e-01,\n",
      "         1.6761e+00,  1.0665e+00, -7.9138e-01, -1.4152e+00, -1.7656e+00,\n",
      "        -6.4179e-01, -8.6015e-01, -3.0178e-01, -1.2543e+00,  1.3319e-02,\n",
      "         2.6493e-01,  3.1750e-01, -2.9320e-01,  1.3305e-01, -2.8118e-01,\n",
      "        -1.4557e+00,  5.3686e-01,  1.7515e-01,  1.4402e+00, -1.8845e+00,\n",
      "         6.2014e-01,  1.1960e+00, -1.1889e+00, -4.6082e-01, -9.2672e-01,\n",
      "        -6.6225e-01, -1.6277e+00,  4.4728e-01,  2.2778e-02,  8.6686e-01,\n",
      "         2.3412e-01, -1.9223e+00,  2.9081e+00,  3.0486e-01,  3.7339e-01,\n",
      "        -1.7093e+00, -1.3273e-01,  2.0712e+00,  2.1455e+00, -2.3169e-01,\n",
      "        -4.6195e-01,  2.2150e-01, -1.7102e-01, -2.4598e+00,  9.3974e-01,\n",
      "        -1.5009e-01,  1.3621e+00,  1.4686e+00,  4.3231e-01,  9.8705e-01,\n",
      "         4.4794e-01, -4.9100e-01,  1.1724e+00,  1.5778e+00, -7.4186e-01,\n",
      "         1.1833e+00,  4.6563e-01, -4.5086e-01, -1.3258e+00,  8.4711e-01,\n",
      "        -1.9602e+00,  1.0661e+00, -3.8956e-02, -4.0217e-01, -1.6831e+00,\n",
      "        -6.8404e-01,  8.7235e-01, -8.1759e-01, -9.9699e-01,  8.4205e-01,\n",
      "         6.7720e-01, -1.5960e-01, -7.7907e-01,  3.1369e-01, -1.1414e+00,\n",
      "        -1.0504e+00,  3.3357e-01, -1.1162e+00,  2.4056e+00, -1.0588e+00,\n",
      "         5.0992e-01, -1.1950e+00,  1.8989e+00, -1.7533e+00, -1.6701e+00,\n",
      "        -1.3328e-01,  1.1068e+00, -2.3427e+00,  1.5110e+00, -1.4099e+00,\n",
      "        -4.8451e-01, -1.2314e-01, -3.3811e-01, -1.3034e+00, -1.1619e+00,\n",
      "         1.1930e+00,  5.1516e-01,  5.6486e-01,  2.6486e-01,  5.9650e-01,\n",
      "         2.7660e-01, -4.4422e-01,  8.6236e-01, -2.9627e-01,  4.5614e-01,\n",
      "        -3.4857e-01,  1.0113e+00,  8.9826e-01,  1.8469e+00, -1.2092e+00,\n",
      "        -6.5184e-01,  5.9022e-01, -5.3727e-01,  6.3065e-01,  1.4172e+00,\n",
      "        -2.6197e-01,  1.7615e+00,  9.2834e-01,  1.4770e+00, -1.3900e+00,\n",
      "        -1.5056e+00,  4.0472e-01,  1.4162e-01, -4.8311e-01,  9.3052e-01,\n",
      "         7.5846e-02, -3.5380e-01,  1.1132e+00, -1.3591e+00, -1.4481e+00,\n",
      "        -3.7084e-01, -1.7373e+00,  5.6152e-01,  1.2964e+00,  7.5565e-01,\n",
      "        -3.5615e-01,  5.7279e-01,  2.5200e+00,  1.3877e+00,  1.9937e+00,\n",
      "        -9.9077e-02,  6.6234e-01, -4.0274e-01,  5.2085e-01, -1.3485e+00,\n",
      "        -4.9809e-01, -1.4101e+00,  1.7063e+00, -1.8624e+00,  9.5977e-01,\n",
      "         9.3663e-01,  6.8436e-03, -9.8793e-01,  9.2543e-01, -1.9597e-01,\n",
      "         7.4683e-01, -1.4896e+00, -2.8135e-01,  1.7183e-01, -5.0299e-01,\n",
      "         9.5310e-02,  5.3671e-01,  4.1958e-01, -7.8673e-01,  1.5775e+00,\n",
      "         6.0812e-01,  9.3524e-01, -9.9641e-01,  1.0869e-01,  4.5402e-03,\n",
      "         2.5965e+00,  3.2071e-01, -1.1195e+00, -2.4650e-01, -7.4536e-02,\n",
      "         3.5852e-01,  4.5684e-01,  6.7013e-01,  1.2076e+00,  2.3457e+00,\n",
      "         8.2339e-02,  3.1463e-01, -1.1040e-01, -1.9040e+00,  1.3739e+00,\n",
      "         3.2325e-01,  1.1880e+00,  1.9464e+00,  2.6254e-01, -1.4052e+00,\n",
      "         1.1917e+00, -8.7038e-01, -6.2783e-01, -8.9141e-03, -2.2773e+00,\n",
      "        -6.4108e-01, -2.4971e+00, -1.5578e-01, -1.8186e-01,  9.2796e-02,\n",
      "         4.2242e-01, -2.5936e-01, -7.0514e-01, -4.5804e-01, -1.1997e+00,\n",
      "        -7.0987e-01, -1.9989e-01,  1.7101e+00, -5.2291e-01, -1.9184e+00,\n",
      "        -7.6437e-01,  7.2374e-01, -9.2071e-01, -3.6503e-01, -1.0471e+00,\n",
      "         3.3801e-01,  1.3734e+00, -1.1466e+00, -1.8853e+00,  4.5289e-01,\n",
      "        -7.6740e-01, -2.0285e-01,  1.4464e+00, -2.0303e+01, -1.8241e+00,\n",
      "         4.0509e-01, -6.5194e-01, -1.6007e+00,  1.2075e+00, -1.4291e+00,\n",
      "        -1.6203e-01,  6.9706e-01, -8.8625e-02, -5.0613e-01,  1.2981e+00,\n",
      "        -4.9296e-01,  3.7897e-01,  3.0108e-01, -8.0881e-01, -2.7239e-02,\n",
      "         6.4522e-01,  1.3926e-01,  9.0030e-01,  1.1285e+00,  1.6351e+00,\n",
      "         3.7097e-01,  1.0222e+00, -2.3408e+00,  6.4330e-01,  4.5735e-01,\n",
      "         7.1323e-01,  2.2841e-01,  9.4996e-01,  8.2625e-01,  1.6350e+00,\n",
      "        -1.4997e+00, -9.8421e-01, -4.9364e-02,  7.2988e-01, -1.5181e-01,\n",
      "         4.4195e-01,  2.4504e-01, -1.4635e-01, -1.4544e+00,  5.5998e-01,\n",
      "        -5.2773e-01, -1.1908e+00,  2.2499e-01,  8.4381e-01,  1.8500e-01,\n",
      "         4.7181e-01,  1.1649e+00,  1.8270e-02,  8.2068e-02,  3.0742e-01,\n",
      "         5.3385e-02, -1.1773e+00, -2.1517e-01, -4.6698e-02, -6.1409e-01,\n",
      "        -8.7967e-01, -5.2338e-01,  1.1350e+00, -5.7997e-01,  1.2927e+00,\n",
      "        -1.9945e-01, -3.3841e-01,  4.2942e-01, -3.6479e-01, -1.2913e+00,\n",
      "         6.3204e-01,  2.8289e-01,  3.9683e-01, -1.2157e+00,  1.0214e+00,\n",
      "         5.3008e-01,  7.5337e-01, -2.8408e-01, -8.4306e-02, -7.5831e-01,\n",
      "        -4.2894e-01, -9.5914e-01, -1.5162e+00,  7.9792e-01,  6.9952e-02,\n",
      "        -7.9060e-01, -6.7528e-01, -7.7009e-01,  7.4460e-01,  7.0786e-01,\n",
      "        -1.8911e-01, -1.0589e+00,  1.1733e+00, -5.4222e-01,  2.1124e-01,\n",
      "        -3.1296e-01, -1.7428e+00,  1.9919e-01,  3.9938e-01,  2.1822e+00,\n",
      "         7.2832e-01, -1.7481e-01, -2.6544e+00, -1.3827e-01, -7.1901e-01,\n",
      "         3.1857e+00,  5.2209e-01, -5.2451e-03, -3.6543e-01,  2.4893e-01,\n",
      "        -4.2261e-01,  3.7183e-03,  1.2674e+00, -1.3796e+00, -2.7210e-01,\n",
      "         4.4761e-01, -7.4976e-01, -1.8547e+00, -7.4264e-01, -1.5748e+00,\n",
      "         5.6605e-01,  3.4418e-01,  1.2235e-01, -1.3920e+00,  1.2800e+00,\n",
      "        -1.5030e+00, -5.0157e-02, -7.2704e-01, -3.0334e-02,  1.5986e+00,\n",
      "         2.5532e+00, -8.0997e-01,  7.1340e-01, -9.6548e-01,  1.1619e+00,\n",
      "         3.1391e-01, -2.0131e+00, -5.7543e-01, -5.2988e-01, -9.4466e-01,\n",
      "        -5.6727e-02, -9.8752e-01, -1.6845e+00, -1.5928e+00,  1.1204e-01,\n",
      "         3.4454e-01,  1.6028e+00,  8.2930e-01,  8.2843e-01,  7.5973e-01,\n",
      "         9.8615e-02, -6.8335e-01, -1.5534e+00, -1.8813e+00,  5.8988e-02,\n",
      "        -2.2060e+00,  8.5155e-01,  2.0750e+00,  8.3085e-01,  6.7677e-01,\n",
      "         1.8948e+00,  4.2101e-01, -7.6151e-02, -1.1608e+00, -1.6170e-01,\n",
      "        -2.7366e+00, -8.6513e-01,  7.8979e-01, -9.5143e-01,  1.7775e-01,\n",
      "        -8.3188e-01, -1.3800e-01,  3.0319e-01,  8.2327e-01, -1.6217e+00,\n",
      "        -2.6038e+00,  1.2568e+00,  1.9402e-01,  1.2261e+00,  6.6277e-01,\n",
      "        -1.2289e+00, -1.6113e+00,  1.3765e+00, -3.4382e-01,  8.5488e-01,\n",
      "         6.7653e-01, -2.9826e-01,  7.6885e-01,  1.5246e+00,  1.6170e+00,\n",
      "         3.7914e-01, -1.3136e+00,  4.3953e-01, -1.8010e+00,  9.8289e-01,\n",
      "         1.1967e+00, -7.8175e-01, -5.0916e-01, -2.1477e-01, -7.5427e-01,\n",
      "         1.4178e+00,  1.0567e-01,  1.2079e+00,  1.0293e+00, -6.9998e-01,\n",
      "         9.7197e-01, -8.2742e-01,  8.0186e-01, -1.2196e+00,  8.3765e-01,\n",
      "        -5.7837e-01, -5.2344e-01,  3.2590e-01,  8.3947e-01,  1.0007e-01,\n",
      "         1.2294e+00,  2.4017e-01, -1.3853e+00, -5.9278e-03,  8.1718e-02,\n",
      "        -4.0815e-01,  1.1438e+00,  4.8126e-01, -1.6397e+00,  7.7685e-01,\n",
      "         1.3722e+00,  1.0995e-01, -1.2662e+00, -9.3301e-01, -1.0122e+00,\n",
      "        -6.3121e-01, -8.9937e-01, -5.7643e-01, -5.6182e-01,  4.0134e-02,\n",
      "         2.7113e-01,  7.5141e-01, -1.7339e+00,  6.9347e-01,  5.2558e-01,\n",
      "        -4.7853e-01,  1.3544e+00,  5.3693e-01,  1.8483e-01, -2.3405e-01,\n",
      "         9.9950e-01,  8.3071e-01,  4.2788e-01, -6.8893e-01,  1.3498e+00,\n",
      "         3.9683e-01, -2.1013e+00, -2.1332e-01,  8.5427e-01,  1.3549e+00,\n",
      "        -6.8388e-01, -1.2997e+00,  6.2665e-01, -8.6118e-01,  4.3876e-01,\n",
      "        -1.5218e+00, -2.2747e-01, -6.0922e-01,  1.1804e+00, -1.5406e+00,\n",
      "         1.7296e-02, -1.0810e+00,  4.0425e-01,  2.5887e-01,  7.8888e-01,\n",
      "        -1.9942e+00,  1.1675e+00,  1.0293e+00,  1.2642e+00, -2.2984e+00,\n",
      "         6.0195e-01, -2.7254e-01, -3.5974e-01,  4.9995e-01,  1.0730e+00,\n",
      "        -2.3406e+00,  3.0183e-02,  3.2447e-01,  3.3247e-01, -4.3546e-01,\n",
      "        -2.6674e-01,  8.3694e-01,  2.1259e-01, -1.6027e-01, -8.0600e-01,\n",
      "        -9.3133e-02,  1.1472e+00,  5.7815e-01, -5.5955e-01, -1.1224e+00,\n",
      "        -5.6802e-01,  2.4624e+00,  9.4765e-01, -6.3948e-01,  2.9447e+00,\n",
      "        -1.2465e+00, -8.6971e-01, -1.9416e-01, -4.5780e-01, -1.7327e+00,\n",
      "         5.7371e-01, -1.2820e+00,  9.7048e-01, -5.4433e-01, -3.7052e-01,\n",
      "         2.9749e-02, -7.8493e-01,  2.2488e+00,  4.2450e-01, -3.1786e-01,\n",
      "         1.0007e+00, -7.2808e-01, -1.7194e+00, -8.5678e-01, -8.2574e-03,\n",
      "         1.7445e+00,  5.8327e-01, -5.6414e-01, -8.1110e-01,  8.3423e-02,\n",
      "        -4.6627e-01, -6.6462e-01, -2.2825e-01,  4.8235e-01,  3.5772e-01,\n",
      "        -1.0459e+00, -1.4159e+00, -4.2098e-01, -6.2110e-01, -1.7127e-01,\n",
      "        -1.3442e+00, -1.5448e-01, -1.7869e-01, -1.0972e+00,  3.4365e-01,\n",
      "        -7.9603e-01,  1.2430e-01,  1.4744e+00, -5.9724e-01,  1.7724e+00,\n",
      "         1.2843e+00, -1.6581e+00, -4.4283e-01, -1.5155e+00,  1.8787e-01,\n",
      "         1.9902e+00,  4.2336e-01, -8.5265e-01,  1.0442e-01, -1.8042e+00,\n",
      "         1.1986e+00,  1.4233e+00,  1.7525e+00, -9.3856e-01,  1.0998e+00,\n",
      "         1.0455e+00,  1.7651e+00,  2.0648e-01,  9.6390e-01,  3.9572e-01,\n",
      "         1.2776e+00, -8.9867e-02,  1.7679e+00,  3.7909e-01,  1.2644e-01,\n",
      "         1.2037e+00, -1.3192e+00,  1.6383e+00, -9.2942e-01, -1.6082e-01,\n",
      "        -5.7921e-01, -1.4099e+00,  2.1181e-01,  2.2842e+00, -2.4947e-01,\n",
      "        -7.4995e-01, -5.7553e-01,  8.2371e-01,  1.8010e+00, -1.0344e+00,\n",
      "         1.0165e+00, -7.6658e-01, -6.9425e-03, -2.0725e+00, -1.4648e+00,\n",
      "        -4.5861e-01, -4.5174e-01,  2.6041e-01, -5.3898e-01, -2.3236e-01,\n",
      "        -3.8218e-01, -1.0867e-01,  5.7270e-01, -1.6870e+00,  2.7395e+00,\n",
      "        -1.9355e+00,  1.4618e+00, -1.5718e-01,  8.4576e-01,  1.2474e+00,\n",
      "         6.9978e-02, -2.9655e-01, -3.6607e-01,  5.0987e-01, -1.1594e+00,\n",
      "        -9.9251e-01, -6.4890e-01,  6.0403e-01], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(40.3134, device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_hidden_embeddings[9])\n",
    "torch.norm(all_hidden_embeddings[6] - all_hidden_embeddings[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT IS:  Once upon a time, there was a girl named Mia. Mia loved her jewelry.\n",
      "She had a big box full of pretty things. She liked to wear them all\n",
      "day. But at night, she had to sleep.  One day, Mia met a talking cat\n",
      "named Tom. Tom was a tough cat, but he was nice. Tom said, \"Hi, Mia! I\n",
      "like your jewelry. Can I wear some too?\" Mia said, \"Yes, Tom. You can\n",
      "wear my jewelry, but we have to give it back before we sleep.\"  So,\n",
      "Mia and Tom played together. They wore the jewelry and had fun. They\n",
      "pretended to be kings and queens. They laughed and danced. But soon,\n",
      "the sun went down, and it was time for bed.  Mia said, \"Tom, we must\n",
      "give back the jewelry now. It's time to sleep.\" Tom gave back the\n",
      "jewelry and said, \"Thank you, Mia. I had fun today.\" They put the\n",
      "jewelry back in the box and went to sleep. Mia and Tom were happy, and\n",
      "they had sweet dreams.\n",
      "CLOSE TEXT IS:  Once upon a time, there was a little girl named Lily. She liked to\n",
      "play outside in the sunshine. One day, she saw a butterfly. The\n",
      "butterfly was very pretty and had many colors.   Lily said, \"Hello,\n",
      "butterfly! Can I play with you?\"   The butterfly said, \"Yes, you can\n",
      "play with me. But I can't stay too long. I will melt if I stay in the\n",
      "sun too much.\"   Lily said, \"Oh no! I don't want you to melt. Let's go\n",
      "find some shade.\"   They found a tree with lots of shade. The\n",
      "butterfly said, \"Thank you for finding a shady spot for me. You are\n",
      "very kind.\"   Lily smiled and said, \"You're welcome. I like to help\n",
      "insects when they need it. I have some water available if you're\n",
      "thirsty.\"   The butterfly drank the water and then flew away. Lily was\n",
      "happy that she could help the butterfly. She went back to playing\n",
      "outside, feeling proud of herself for being kind to the little insect.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "def wrap(line):\n",
    "    broken = textwrap.wrap(line,70, break_long_words=False)\n",
    "    #print('broken is ', broken)\n",
    "    return '\\n'.join(broken)\n",
    "\n",
    "w_input_text = wrap(input_text)\n",
    "w_close_text = wrap(close_text)\n",
    "print('INPUT TEXT IS: ', w_input_text)\n",
    "print('CLOSE TEXT IS: ', w_close_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### obtaining the average of the embeddings\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "t5tokenizer = T5Tokenizer.from_pretrained('google-t5/t5-small')\n",
    "t5model = T5EncoderModel.from_pretrained('google-t5/t5-small')\n",
    "input_ids = t5tokenizer(\n",
    "    'Studies have been shown that owning a dog is good for you', return_tensors='pt'\n",
    ").input_ids\n",
    "outputs = t5model.encoder(input_ids)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "last_hidden_states.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5824, 0.1530, 0.1705])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      3\u001b[0m num_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_text \u001b[38;5;241m/\u001b[39m batch_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 20\n",
    "num_batch = int(num_text / batch_size)\n",
    "all_hidden_embeddings = torch.empty(0, model.config.hidden_size).to(device)\n",
    "with torch.no_grad():\n",
    "    for i in range(num_batch):\n",
    "        #print(len(model.encoder(torch.tensor(tokenized_dataset['input_ids'])[i*batch_size:(i+1)*batch_size,:].to(device))))\n",
    "        #print('salam')\n",
    "        hidden_embeddings = t5model(torch.tensor(tokenized_dataset['input_ids'])[i*batch_size:(i+1)*batch_size,:].to(device)).last_hidden_state[np.arange(batch_size), tokenized_dataset['length'][i*batch_size:(i+1)*batch_size], :] \n",
    "        all_hidden_embeddings = torch.cat((all_hidden_embeddings, hidden_embeddings), dim=0)\n",
    "all_hidden_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat as nbf\n",
    "with open('knn-search.ipynb', 'r') as f:\n",
    "    nb = nbf.read(f, as_version=4)\n",
    "with open('./knn-search/two-similar-stories-found-within-500-stories.ipynb', 'w') as f:\n",
    "    nbf.write(nb, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
