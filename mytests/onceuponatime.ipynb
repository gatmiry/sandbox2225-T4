{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mymodel\n",
    "from transformers import AutoTokenizer, GPT2Model\n",
    "import torch\n",
    "#checkpoint = './testmodel3weights_2024-07-07--05:59:32'\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyModel were not initialized from the model checkpoint at ./backmodel3_weights_2024-07-24--19:55:58alaki and are newly initialized: ['decoder.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = './backmodel3_weights_2024-07-24--19:55:58alaki'\n",
    "model = mymodel.MyModel.from_pretrained(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_input = 'Once upon a time, there was a big car named Dependable.'\n",
    "second_input_ids = tokenizer(second_input, return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_state_dict = model.state_dict()\n",
    "#torch.save(model_state_dict, './modelalaki')\n",
    "\n",
    "### loading model from pydict\n",
    "from transformers import AutoConfig, AutoModel\n",
    "import safetensors\n",
    "#state_dict = torch.load('./nnmodel3_weights_2024-07-16--01:57:44')\n",
    "state_dict = torch.load('./nnmodel3_weights_2024-07-17--04:24:10')\n",
    "#state_dict = torch.load('./nnmodel3_weights_2024-07-24--16:22:27alaki')\n",
    "\n",
    "\n",
    "#state_dict = torch.load('./nnmodel3_default_outputs_2024-07-17--04:24:10/checkpoint-30000/model.safetensors')\n",
    "\n",
    "#mymodel = safetensors.torch.load('./nnmodel3_default_outputs_2024-07-17--04:24:10/checkpoint-30000/model.safetensors')\n",
    "\n",
    "#mymodel3  = AutoModel.from_pretrained('./nnmodel3_default_outputs_2024-07-17--04:24:10/checkpoint-30000')\n",
    "\n",
    "myconfig = AutoConfig.from_pretrained('gpt2')\n",
    "myconfig2 = AutoConfig.from_pretrained('gpt2')\n",
    "context_length=512\n",
    "myconfig.n_ctx = context_length\n",
    "model2 = mymodel.MyModel(myconfig)\n",
    "model2.load_state_dict(state_dict)\n",
    "randmodel = mymodel.MyModel(myconfig2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = '''Once upon a time, there was a big car named Dependable. He had a very important job. Dependable would take a family to the park every day. The family had a mom, dad, and a little girl named Lily. They all had a lot of love for each other.\\n\\nOne day, when they got to the park, they saw a big sign that said, \"Fun Race Today!\" The family was very excited. They knew that Dependable was very fast and could win the race. So, they decided to join the race.\\n\\nThe race started, and Dependable went very fast. The other cars tried to catch up, but Dependable was too quick. In the end, Dependable won the race! The family was so happy and proud of their car. They knew that their love for each other and their trust in Dependable made them win the race. And from that day on, they had even more fun at the park, knowing that they had the fastest and most dependable car around.'''\n",
    "input_ids = tokenizer(input_str, return_tensors='pt')['input_ids']\n",
    "hidden_embedding = model2.encoder(input_ids).last_hidden_state.detach()[0, -1, :].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_873995/26876073.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hidden_embedding = model2.encoder(torch.tensor(input_ids)).last_hidden_state.detach()[0, -1, :].unsqueeze(0)\n",
      "/tmp/ipykernel_873995/26876073.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_hidden_inputs = model2.second_encoder(torch.tensor(second_input_ids), output_hidden_states=True).hidden_states[0][0].detach()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## don't run this!\n",
    "hidden_embedding = model2.encoder(torch.tensor(input_ids)).last_hidden_state.detach()[0, -1, :].unsqueeze(0)\n",
    "\n",
    "## now create the new input to the decoder, first obtain the word embeddings using a pretrained gpt2 model\n",
    "second_input = 'Once upon a time, there was an elderly farmer, who had '\n",
    "second_input_ids = tokenizer(second_input, return_tensors='pt')['input_ids']\n",
    "decoder_hidden_inputs = model2.second_encoder(torch.tensor(second_input_ids), output_hidden_states=True).hidden_states[0][0].detach()\n",
    "#print('shape is ', decoder_hidden_outputs.shape)\n",
    "\n",
    "#second_input_hidden_embeddings = model.second_encoder(tokenizer(second_input, return_tensors='pt')['input_ids']).last_hidden_states[0][0]\n",
    "\n",
    "updated_input = torch.cat((hidden_embedding, decoder_hidden_inputs), dim=0).unsqueeze(0)\n",
    "updated_input.shape\n",
    "#torch.zeros((439, 768))\n",
    "#model.decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are  dict_keys(['decoder.transformer.h.0.attn.c_attn.bias', 'decoder.transformer.h.0.attn.c_attn.weight', 'decoder.transformer.h.0.attn.c_proj.bias', 'decoder.transformer.h.0.attn.c_proj.weight', 'decoder.transformer.h.0.ln_1.bias', 'decoder.transformer.h.0.ln_1.weight', 'decoder.transformer.h.0.ln_2.bias', 'decoder.transformer.h.0.ln_2.weight', 'decoder.transformer.h.0.mlp.c_fc.bias', 'decoder.transformer.h.0.mlp.c_fc.weight', 'decoder.transformer.h.0.mlp.c_proj.bias', 'decoder.transformer.h.0.mlp.c_proj.weight', 'decoder.transformer.h.1.attn.c_attn.bias', 'decoder.transformer.h.1.attn.c_attn.weight', 'decoder.transformer.h.1.attn.c_proj.bias', 'decoder.transformer.h.1.attn.c_proj.weight', 'decoder.transformer.h.1.ln_1.bias', 'decoder.transformer.h.1.ln_1.weight', 'decoder.transformer.h.1.ln_2.bias', 'decoder.transformer.h.1.ln_2.weight', 'decoder.transformer.h.1.mlp.c_fc.bias', 'decoder.transformer.h.1.mlp.c_fc.weight', 'decoder.transformer.h.1.mlp.c_proj.bias', 'decoder.transformer.h.1.mlp.c_proj.weight', 'decoder.transformer.h.10.attn.c_attn.bias', 'decoder.transformer.h.10.attn.c_attn.weight', 'decoder.transformer.h.10.attn.c_proj.bias', 'decoder.transformer.h.10.attn.c_proj.weight', 'decoder.transformer.h.10.ln_1.bias', 'decoder.transformer.h.10.ln_1.weight', 'decoder.transformer.h.10.ln_2.bias', 'decoder.transformer.h.10.ln_2.weight', 'decoder.transformer.h.10.mlp.c_fc.bias', 'decoder.transformer.h.10.mlp.c_fc.weight', 'decoder.transformer.h.10.mlp.c_proj.bias', 'decoder.transformer.h.10.mlp.c_proj.weight', 'decoder.transformer.h.11.attn.c_attn.bias', 'decoder.transformer.h.11.attn.c_attn.weight', 'decoder.transformer.h.11.attn.c_proj.bias', 'decoder.transformer.h.11.attn.c_proj.weight', 'decoder.transformer.h.11.ln_1.bias', 'decoder.transformer.h.11.ln_1.weight', 'decoder.transformer.h.11.ln_2.bias', 'decoder.transformer.h.11.ln_2.weight', 'decoder.transformer.h.11.mlp.c_fc.bias', 'decoder.transformer.h.11.mlp.c_fc.weight', 'decoder.transformer.h.11.mlp.c_proj.bias', 'decoder.transformer.h.11.mlp.c_proj.weight', 'decoder.transformer.h.2.attn.c_attn.bias', 'decoder.transformer.h.2.attn.c_attn.weight', 'decoder.transformer.h.2.attn.c_proj.bias', 'decoder.transformer.h.2.attn.c_proj.weight', 'decoder.transformer.h.2.ln_1.bias', 'decoder.transformer.h.2.ln_1.weight', 'decoder.transformer.h.2.ln_2.bias', 'decoder.transformer.h.2.ln_2.weight', 'decoder.transformer.h.2.mlp.c_fc.bias', 'decoder.transformer.h.2.mlp.c_fc.weight', 'decoder.transformer.h.2.mlp.c_proj.bias', 'decoder.transformer.h.2.mlp.c_proj.weight', 'decoder.transformer.h.3.attn.c_attn.bias', 'decoder.transformer.h.3.attn.c_attn.weight', 'decoder.transformer.h.3.attn.c_proj.bias', 'decoder.transformer.h.3.attn.c_proj.weight', 'decoder.transformer.h.3.ln_1.bias', 'decoder.transformer.h.3.ln_1.weight', 'decoder.transformer.h.3.ln_2.bias', 'decoder.transformer.h.3.ln_2.weight', 'decoder.transformer.h.3.mlp.c_fc.bias', 'decoder.transformer.h.3.mlp.c_fc.weight', 'decoder.transformer.h.3.mlp.c_proj.bias', 'decoder.transformer.h.3.mlp.c_proj.weight', 'decoder.transformer.h.4.attn.c_attn.bias', 'decoder.transformer.h.4.attn.c_attn.weight', 'decoder.transformer.h.4.attn.c_proj.bias', 'decoder.transformer.h.4.attn.c_proj.weight', 'decoder.transformer.h.4.ln_1.bias', 'decoder.transformer.h.4.ln_1.weight', 'decoder.transformer.h.4.ln_2.bias', 'decoder.transformer.h.4.ln_2.weight', 'decoder.transformer.h.4.mlp.c_fc.bias', 'decoder.transformer.h.4.mlp.c_fc.weight', 'decoder.transformer.h.4.mlp.c_proj.bias', 'decoder.transformer.h.4.mlp.c_proj.weight', 'decoder.transformer.h.5.attn.c_attn.bias', 'decoder.transformer.h.5.attn.c_attn.weight', 'decoder.transformer.h.5.attn.c_proj.bias', 'decoder.transformer.h.5.attn.c_proj.weight', 'decoder.transformer.h.5.ln_1.bias', 'decoder.transformer.h.5.ln_1.weight', 'decoder.transformer.h.5.ln_2.bias', 'decoder.transformer.h.5.ln_2.weight', 'decoder.transformer.h.5.mlp.c_fc.bias', 'decoder.transformer.h.5.mlp.c_fc.weight', 'decoder.transformer.h.5.mlp.c_proj.bias', 'decoder.transformer.h.5.mlp.c_proj.weight', 'decoder.transformer.h.6.attn.c_attn.bias', 'decoder.transformer.h.6.attn.c_attn.weight', 'decoder.transformer.h.6.attn.c_proj.bias', 'decoder.transformer.h.6.attn.c_proj.weight', 'decoder.transformer.h.6.ln_1.bias', 'decoder.transformer.h.6.ln_1.weight', 'decoder.transformer.h.6.ln_2.bias', 'decoder.transformer.h.6.ln_2.weight', 'decoder.transformer.h.6.mlp.c_fc.bias', 'decoder.transformer.h.6.mlp.c_fc.weight', 'decoder.transformer.h.6.mlp.c_proj.bias', 'decoder.transformer.h.6.mlp.c_proj.weight', 'decoder.transformer.h.7.attn.c_attn.bias', 'decoder.transformer.h.7.attn.c_attn.weight', 'decoder.transformer.h.7.attn.c_proj.bias', 'decoder.transformer.h.7.attn.c_proj.weight', 'decoder.transformer.h.7.ln_1.bias', 'decoder.transformer.h.7.ln_1.weight', 'decoder.transformer.h.7.ln_2.bias', 'decoder.transformer.h.7.ln_2.weight', 'decoder.transformer.h.7.mlp.c_fc.bias', 'decoder.transformer.h.7.mlp.c_fc.weight', 'decoder.transformer.h.7.mlp.c_proj.bias', 'decoder.transformer.h.7.mlp.c_proj.weight', 'decoder.transformer.h.8.attn.c_attn.bias', 'decoder.transformer.h.8.attn.c_attn.weight', 'decoder.transformer.h.8.attn.c_proj.bias', 'decoder.transformer.h.8.attn.c_proj.weight', 'decoder.transformer.h.8.ln_1.bias', 'decoder.transformer.h.8.ln_1.weight', 'decoder.transformer.h.8.ln_2.bias', 'decoder.transformer.h.8.ln_2.weight', 'decoder.transformer.h.8.mlp.c_fc.bias', 'decoder.transformer.h.8.mlp.c_fc.weight', 'decoder.transformer.h.8.mlp.c_proj.bias', 'decoder.transformer.h.8.mlp.c_proj.weight', 'decoder.transformer.h.9.attn.c_attn.bias', 'decoder.transformer.h.9.attn.c_attn.weight', 'decoder.transformer.h.9.attn.c_proj.bias', 'decoder.transformer.h.9.attn.c_proj.weight', 'decoder.transformer.h.9.ln_1.bias', 'decoder.transformer.h.9.ln_1.weight', 'decoder.transformer.h.9.ln_2.bias', 'decoder.transformer.h.9.ln_2.weight', 'decoder.transformer.h.9.mlp.c_fc.bias', 'decoder.transformer.h.9.mlp.c_fc.weight', 'decoder.transformer.h.9.mlp.c_proj.bias', 'decoder.transformer.h.9.mlp.c_proj.weight', 'decoder.transformer.ln_f.bias', 'decoder.transformer.ln_f.weight', 'decoder.transformer.wpe.weight', 'decoder.transformer.wte.weight', 'encoder.h.0.attn.c_attn.bias', 'encoder.h.0.attn.c_attn.weight', 'encoder.h.0.attn.c_proj.bias', 'encoder.h.0.attn.c_proj.weight', 'encoder.h.0.ln_1.bias', 'encoder.h.0.ln_1.weight', 'encoder.h.0.ln_2.bias', 'encoder.h.0.ln_2.weight', 'encoder.h.0.mlp.c_fc.bias', 'encoder.h.0.mlp.c_fc.weight', 'encoder.h.0.mlp.c_proj.bias', 'encoder.h.0.mlp.c_proj.weight', 'encoder.h.1.attn.c_attn.bias', 'encoder.h.1.attn.c_attn.weight', 'encoder.h.1.attn.c_proj.bias', 'encoder.h.1.attn.c_proj.weight', 'encoder.h.1.ln_1.bias', 'encoder.h.1.ln_1.weight', 'encoder.h.1.ln_2.bias', 'encoder.h.1.ln_2.weight', 'encoder.h.1.mlp.c_fc.bias', 'encoder.h.1.mlp.c_fc.weight', 'encoder.h.1.mlp.c_proj.bias', 'encoder.h.1.mlp.c_proj.weight', 'encoder.h.10.attn.c_attn.bias', 'encoder.h.10.attn.c_attn.weight', 'encoder.h.10.attn.c_proj.bias', 'encoder.h.10.attn.c_proj.weight', 'encoder.h.10.ln_1.bias', 'encoder.h.10.ln_1.weight', 'encoder.h.10.ln_2.bias', 'encoder.h.10.ln_2.weight', 'encoder.h.10.mlp.c_fc.bias', 'encoder.h.10.mlp.c_fc.weight', 'encoder.h.10.mlp.c_proj.bias', 'encoder.h.10.mlp.c_proj.weight', 'encoder.h.11.attn.c_attn.bias', 'encoder.h.11.attn.c_attn.weight', 'encoder.h.11.attn.c_proj.bias', 'encoder.h.11.attn.c_proj.weight', 'encoder.h.11.ln_1.bias', 'encoder.h.11.ln_1.weight', 'encoder.h.11.ln_2.bias', 'encoder.h.11.ln_2.weight', 'encoder.h.11.mlp.c_fc.bias', 'encoder.h.11.mlp.c_fc.weight', 'encoder.h.11.mlp.c_proj.bias', 'encoder.h.11.mlp.c_proj.weight', 'encoder.h.2.attn.c_attn.bias', 'encoder.h.2.attn.c_attn.weight', 'encoder.h.2.attn.c_proj.bias', 'encoder.h.2.attn.c_proj.weight', 'encoder.h.2.ln_1.bias', 'encoder.h.2.ln_1.weight', 'encoder.h.2.ln_2.bias', 'encoder.h.2.ln_2.weight', 'encoder.h.2.mlp.c_fc.bias', 'encoder.h.2.mlp.c_fc.weight', 'encoder.h.2.mlp.c_proj.bias', 'encoder.h.2.mlp.c_proj.weight', 'encoder.h.3.attn.c_attn.bias', 'encoder.h.3.attn.c_attn.weight', 'encoder.h.3.attn.c_proj.bias', 'encoder.h.3.attn.c_proj.weight', 'encoder.h.3.ln_1.bias', 'encoder.h.3.ln_1.weight', 'encoder.h.3.ln_2.bias', 'encoder.h.3.ln_2.weight', 'encoder.h.3.mlp.c_fc.bias', 'encoder.h.3.mlp.c_fc.weight', 'encoder.h.3.mlp.c_proj.bias', 'encoder.h.3.mlp.c_proj.weight', 'encoder.h.4.attn.c_attn.bias', 'encoder.h.4.attn.c_attn.weight', 'encoder.h.4.attn.c_proj.bias', 'encoder.h.4.attn.c_proj.weight', 'encoder.h.4.ln_1.bias', 'encoder.h.4.ln_1.weight', 'encoder.h.4.ln_2.bias', 'encoder.h.4.ln_2.weight', 'encoder.h.4.mlp.c_fc.bias', 'encoder.h.4.mlp.c_fc.weight', 'encoder.h.4.mlp.c_proj.bias', 'encoder.h.4.mlp.c_proj.weight', 'encoder.h.5.attn.c_attn.bias', 'encoder.h.5.attn.c_attn.weight', 'encoder.h.5.attn.c_proj.bias', 'encoder.h.5.attn.c_proj.weight', 'encoder.h.5.ln_1.bias', 'encoder.h.5.ln_1.weight', 'encoder.h.5.ln_2.bias', 'encoder.h.5.ln_2.weight', 'encoder.h.5.mlp.c_fc.bias', 'encoder.h.5.mlp.c_fc.weight', 'encoder.h.5.mlp.c_proj.bias', 'encoder.h.5.mlp.c_proj.weight', 'encoder.h.6.attn.c_attn.bias', 'encoder.h.6.attn.c_attn.weight', 'encoder.h.6.attn.c_proj.bias', 'encoder.h.6.attn.c_proj.weight', 'encoder.h.6.ln_1.bias', 'encoder.h.6.ln_1.weight', 'encoder.h.6.ln_2.bias', 'encoder.h.6.ln_2.weight', 'encoder.h.6.mlp.c_fc.bias', 'encoder.h.6.mlp.c_fc.weight', 'encoder.h.6.mlp.c_proj.bias', 'encoder.h.6.mlp.c_proj.weight', 'encoder.h.7.attn.c_attn.bias', 'encoder.h.7.attn.c_attn.weight', 'encoder.h.7.attn.c_proj.bias', 'encoder.h.7.attn.c_proj.weight', 'encoder.h.7.ln_1.bias', 'encoder.h.7.ln_1.weight', 'encoder.h.7.ln_2.bias', 'encoder.h.7.ln_2.weight', 'encoder.h.7.mlp.c_fc.bias', 'encoder.h.7.mlp.c_fc.weight', 'encoder.h.7.mlp.c_proj.bias', 'encoder.h.7.mlp.c_proj.weight', 'encoder.h.8.attn.c_attn.bias', 'encoder.h.8.attn.c_attn.weight', 'encoder.h.8.attn.c_proj.bias', 'encoder.h.8.attn.c_proj.weight', 'encoder.h.8.ln_1.bias', 'encoder.h.8.ln_1.weight', 'encoder.h.8.ln_2.bias', 'encoder.h.8.ln_2.weight', 'encoder.h.8.mlp.c_fc.bias', 'encoder.h.8.mlp.c_fc.weight', 'encoder.h.8.mlp.c_proj.bias', 'encoder.h.8.mlp.c_proj.weight', 'encoder.h.9.attn.c_attn.bias', 'encoder.h.9.attn.c_attn.weight', 'encoder.h.9.attn.c_proj.bias', 'encoder.h.9.attn.c_proj.weight', 'encoder.h.9.ln_1.bias', 'encoder.h.9.ln_1.weight', 'encoder.h.9.ln_2.bias', 'encoder.h.9.ln_2.weight', 'encoder.h.9.mlp.c_fc.bias', 'encoder.h.9.mlp.c_fc.weight', 'encoder.h.9.mlp.c_proj.bias', 'encoder.h.9.mlp.c_proj.weight', 'encoder.ln_f.bias', 'encoder.ln_f.weight', 'encoder.wpe.weight', 'encoder.wte.weight', 'second_encoder.h.0.attn.c_attn.bias', 'second_encoder.h.0.attn.c_attn.weight', 'second_encoder.h.0.attn.c_proj.bias', 'second_encoder.h.0.attn.c_proj.weight', 'second_encoder.h.0.ln_1.bias', 'second_encoder.h.0.ln_1.weight', 'second_encoder.h.0.ln_2.bias', 'second_encoder.h.0.ln_2.weight', 'second_encoder.h.0.mlp.c_fc.bias', 'second_encoder.h.0.mlp.c_fc.weight', 'second_encoder.h.0.mlp.c_proj.bias', 'second_encoder.h.0.mlp.c_proj.weight', 'second_encoder.h.1.attn.c_attn.bias', 'second_encoder.h.1.attn.c_attn.weight', 'second_encoder.h.1.attn.c_proj.bias', 'second_encoder.h.1.attn.c_proj.weight', 'second_encoder.h.1.ln_1.bias', 'second_encoder.h.1.ln_1.weight', 'second_encoder.h.1.ln_2.bias', 'second_encoder.h.1.ln_2.weight', 'second_encoder.h.1.mlp.c_fc.bias', 'second_encoder.h.1.mlp.c_fc.weight', 'second_encoder.h.1.mlp.c_proj.bias', 'second_encoder.h.1.mlp.c_proj.weight', 'second_encoder.h.10.attn.c_attn.bias', 'second_encoder.h.10.attn.c_attn.weight', 'second_encoder.h.10.attn.c_proj.bias', 'second_encoder.h.10.attn.c_proj.weight', 'second_encoder.h.10.ln_1.bias', 'second_encoder.h.10.ln_1.weight', 'second_encoder.h.10.ln_2.bias', 'second_encoder.h.10.ln_2.weight', 'second_encoder.h.10.mlp.c_fc.bias', 'second_encoder.h.10.mlp.c_fc.weight', 'second_encoder.h.10.mlp.c_proj.bias', 'second_encoder.h.10.mlp.c_proj.weight', 'second_encoder.h.11.attn.c_attn.bias', 'second_encoder.h.11.attn.c_attn.weight', 'second_encoder.h.11.attn.c_proj.bias', 'second_encoder.h.11.attn.c_proj.weight', 'second_encoder.h.11.ln_1.bias', 'second_encoder.h.11.ln_1.weight', 'second_encoder.h.11.ln_2.bias', 'second_encoder.h.11.ln_2.weight', 'second_encoder.h.11.mlp.c_fc.bias', 'second_encoder.h.11.mlp.c_fc.weight', 'second_encoder.h.11.mlp.c_proj.bias', 'second_encoder.h.11.mlp.c_proj.weight', 'second_encoder.h.2.attn.c_attn.bias', 'second_encoder.h.2.attn.c_attn.weight', 'second_encoder.h.2.attn.c_proj.bias', 'second_encoder.h.2.attn.c_proj.weight', 'second_encoder.h.2.ln_1.bias', 'second_encoder.h.2.ln_1.weight', 'second_encoder.h.2.ln_2.bias', 'second_encoder.h.2.ln_2.weight', 'second_encoder.h.2.mlp.c_fc.bias', 'second_encoder.h.2.mlp.c_fc.weight', 'second_encoder.h.2.mlp.c_proj.bias', 'second_encoder.h.2.mlp.c_proj.weight', 'second_encoder.h.3.attn.c_attn.bias', 'second_encoder.h.3.attn.c_attn.weight', 'second_encoder.h.3.attn.c_proj.bias', 'second_encoder.h.3.attn.c_proj.weight', 'second_encoder.h.3.ln_1.bias', 'second_encoder.h.3.ln_1.weight', 'second_encoder.h.3.ln_2.bias', 'second_encoder.h.3.ln_2.weight', 'second_encoder.h.3.mlp.c_fc.bias', 'second_encoder.h.3.mlp.c_fc.weight', 'second_encoder.h.3.mlp.c_proj.bias', 'second_encoder.h.3.mlp.c_proj.weight', 'second_encoder.h.4.attn.c_attn.bias', 'second_encoder.h.4.attn.c_attn.weight', 'second_encoder.h.4.attn.c_proj.bias', 'second_encoder.h.4.attn.c_proj.weight', 'second_encoder.h.4.ln_1.bias', 'second_encoder.h.4.ln_1.weight', 'second_encoder.h.4.ln_2.bias', 'second_encoder.h.4.ln_2.weight', 'second_encoder.h.4.mlp.c_fc.bias', 'second_encoder.h.4.mlp.c_fc.weight', 'second_encoder.h.4.mlp.c_proj.bias', 'second_encoder.h.4.mlp.c_proj.weight', 'second_encoder.h.5.attn.c_attn.bias', 'second_encoder.h.5.attn.c_attn.weight', 'second_encoder.h.5.attn.c_proj.bias', 'second_encoder.h.5.attn.c_proj.weight', 'second_encoder.h.5.ln_1.bias', 'second_encoder.h.5.ln_1.weight', 'second_encoder.h.5.ln_2.bias', 'second_encoder.h.5.ln_2.weight', 'second_encoder.h.5.mlp.c_fc.bias', 'second_encoder.h.5.mlp.c_fc.weight', 'second_encoder.h.5.mlp.c_proj.bias', 'second_encoder.h.5.mlp.c_proj.weight', 'second_encoder.h.6.attn.c_attn.bias', 'second_encoder.h.6.attn.c_attn.weight', 'second_encoder.h.6.attn.c_proj.bias', 'second_encoder.h.6.attn.c_proj.weight', 'second_encoder.h.6.ln_1.bias', 'second_encoder.h.6.ln_1.weight', 'second_encoder.h.6.ln_2.bias', 'second_encoder.h.6.ln_2.weight', 'second_encoder.h.6.mlp.c_fc.bias', 'second_encoder.h.6.mlp.c_fc.weight', 'second_encoder.h.6.mlp.c_proj.bias', 'second_encoder.h.6.mlp.c_proj.weight', 'second_encoder.h.7.attn.c_attn.bias', 'second_encoder.h.7.attn.c_attn.weight', 'second_encoder.h.7.attn.c_proj.bias', 'second_encoder.h.7.attn.c_proj.weight', 'second_encoder.h.7.ln_1.bias', 'second_encoder.h.7.ln_1.weight', 'second_encoder.h.7.ln_2.bias', 'second_encoder.h.7.ln_2.weight', 'second_encoder.h.7.mlp.c_fc.bias', 'second_encoder.h.7.mlp.c_fc.weight', 'second_encoder.h.7.mlp.c_proj.bias', 'second_encoder.h.7.mlp.c_proj.weight', 'second_encoder.h.8.attn.c_attn.bias', 'second_encoder.h.8.attn.c_attn.weight', 'second_encoder.h.8.attn.c_proj.bias', 'second_encoder.h.8.attn.c_proj.weight', 'second_encoder.h.8.ln_1.bias', 'second_encoder.h.8.ln_1.weight', 'second_encoder.h.8.ln_2.bias', 'second_encoder.h.8.ln_2.weight', 'second_encoder.h.8.mlp.c_fc.bias', 'second_encoder.h.8.mlp.c_fc.weight', 'second_encoder.h.8.mlp.c_proj.bias', 'second_encoder.h.8.mlp.c_proj.weight', 'second_encoder.h.9.attn.c_attn.bias', 'second_encoder.h.9.attn.c_attn.weight', 'second_encoder.h.9.attn.c_proj.bias', 'second_encoder.h.9.attn.c_proj.weight', 'second_encoder.h.9.ln_1.bias', 'second_encoder.h.9.ln_1.weight', 'second_encoder.h.9.ln_2.bias', 'second_encoder.h.9.ln_2.weight', 'second_encoder.h.9.mlp.c_fc.bias', 'second_encoder.h.9.mlp.c_fc.weight', 'second_encoder.h.9.mlp.c_proj.bias', 'second_encoder.h.9.mlp.c_proj.weight', 'second_encoder.ln_f.bias', 'second_encoder.ln_f.weight', 'second_encoder.wpe.weight', 'second_encoder.wte.weight'])\n"
     ]
    }
   ],
   "source": [
    "#import safetensors.torch \n",
    "#myfile = safetensors.torch.load_file('./testmodel3weights_2024-07-07--05:59:32/model.safetensors')\n",
    "#print('keys are ', myfile.keys())\n",
    "#for item in list(myfile.keys()):\n",
    "#    if 'lm_head' in item:\n",
    "#        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = 0\n",
    "#for param in model.encoder.parameters():\n",
    "#    if index  > 3:\n",
    "#        print('param is ', param)\n",
    "#    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' grand upon a time, there was a elderly couple, with happily been became'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from transformers import GPT2LMHeadModel, AutoConfig\n",
    "#print(updated_input.shape)\n",
    "#decoder_second = GPT2LMHeadModel(myconfig)\n",
    "\n",
    "#model.decoder.output_hidden_states = True\n",
    "#outputs = model2.decoder(input_ids=tokenizer(input_str, return_tensors='pt')['input_ids'])#,  output_hidden_states=True\n",
    "#, attention_mask=tokenizer(input_str, return_tensors='pt')['attention_mask']\n",
    "\n",
    "#print('output logits are ', outputs)\n",
    "#strr = model2.decoder.generate(inputs_embeds = updated_input)\n",
    "#print('strr is ', tokenizer.decode(strr[0]))\n",
    "\n",
    "logits = model2.decoder(inputs_embeds = updated_input)['logits']\n",
    "#model2(input_embeds = )\n",
    "max_index = torch.argmax(logits, dim=2)\n",
    "#max_index\n",
    "#logits.shape\n",
    "decoded_seq = tokenizer.decode(max_index[0])\n",
    "decoded_seq\n",
    "#print('logits are ', logits[2].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7454,  2402,   257,   640,    11,   612,   373,   257,  1263,  1097,\n",
      "          3706, 37947,   540,    13]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_990609/1929015111.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  decoder_hidden_inputs = model2.second_encoder(torch.tensor(second_input_ids), output_hidden_states=True).hidden_states[0][0].detach()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a big car named Dependable. Dependable had a very important job. He liked to go fast. Today, he was going to race with the other cars.\\n\\nThe race began, and Dependable got very happy. He said to a car named Car, \"Let\\'s race, Car!\" Car said, \"OK, let\\'s go!\" They started to race and the dependable was very big.\\n\\nAfter the race, Dependable knew that racing would be fun. The race should finish, but Truck did not want to lose. They raced again, but the race starting.\\n\\nThe race began, and the dependable was very fast. But then, Car\\'s engine broke. The car was in danger of surrendering to its friend, Truckable. Truck was brave and stopped the race. He remembered how dependable and good Car was at the end. So, he won the race with his engine in his heart. The car with its engine was happy and cheered for his friend. They knew that Dependable could win the race and support their friend, because they were there for their friend. Everyone learned from Dependable and knew that their friendship was the best job. They loved to race and play together with their friends in their hearts.</s>Ben and Lily were twins who liked to play soccer. They learned how to kick the ball with their feet and how to score goals in their team. They had a coach named Mr. Fox, who was very good at soccer. He told them that'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_input = 'Once upon a time, there was a big car named Dependable.'\n",
    "second_input_ids = tokenizer(second_input, return_tensors='pt')['input_ids']\n",
    "print(second_input_ids)\n",
    "for i in range(300):\n",
    "    #second_input = 'Once upon a time, there was an elderly farmer'\n",
    "    \n",
    "    decoder_hidden_inputs = model2.second_encoder(torch.tensor(second_input_ids), output_hidden_states=True).hidden_states[0][0].detach()\n",
    "    #print('shape is ', decoder_hidden_outputs.shape)\n",
    "    #second_input_hidden_embeddings = model.second_encoder(tokenizer(second_input, return_tensors='pt')['input_ids']).last_hidden_states[0][0]\n",
    "    updated_input = torch.cat((hidden_embedding, decoder_hidden_inputs), dim=0).unsqueeze(0)\n",
    "    #updated_input = decoder_hidden_inputs.unsqueeze(0)\n",
    "    logits = model2.decoder(inputs_embeds = updated_input)['logits']\n",
    "    #model2(input_embeds = )\n",
    "    max_index = torch.argmax(logits, dim=2)\n",
    "    #max_index\n",
    "    #logits.shape\n",
    "    \n",
    "    #outword = tokenizer.decode(max_index[0][-1])\n",
    "    second_input_ids = torch.cat((second_input_ids, max_index[:,-1:]),  dim=1)\n",
    "    #print('second input is ', second_input)\n",
    "#print(second_input_ids)\n",
    "second_input = tokenizer.decode(second_input_ids[0])\n",
    "second_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9310e+00,  1.6959e+00, -2.0372e-01,  ..., -1.0425e+00,\n",
       "         -2.0197e+00, -1.9180e+00],\n",
       "        [ 7.1667e-02, -1.1264e-02, -3.1603e-02,  ...,  5.5578e-02,\n",
       "          0.0000e+00, -2.1566e-02],\n",
       "        [-2.8862e-02,  0.0000e+00, -1.2996e-03,  ...,  4.9011e-02,\n",
       "         -5.8250e-03,  1.3068e-02],\n",
       "        ...,\n",
       "        [ 3.3293e-02,  3.9718e-02, -1.8351e-02,  ..., -4.3628e-02,\n",
       "         -3.6502e-02, -2.9447e-02],\n",
       "        [-2.9202e-06, -1.7530e-02, -1.8004e-02,  ...,  7.1160e-03,\n",
       "         -3.1151e-03, -3.2737e-02],\n",
       "        [-2.0993e-02, -7.1361e-02,  3.9832e-02,  ...,  1.0748e-02,\n",
       "         -5.2369e-02,  2.5638e-02]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "tinystories =  load_dataset(\"roneneldan/TinyStories\")\n",
    "tinystories\n",
    "from mymodelnew3 import MyModel\n",
    "checkpoint = './posmodel3_weights_2024-08-01--05:36:40alaki'\n",
    "#checkpoint = './posmodel3_weights_2024-07-31--00:22:21alaki'\n",
    "#checkpoint = './testmodel3weights_2024-07-07--05:59:32'\n",
    "#checkpoint = './encoder_is_pretrained_average.py_gpt2tokenizer_2024-07-14--21:42:14/model3weights'\n",
    "model2 = MyModel.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import numpy as np\n",
    "tokenized_datasets = load_from_disk('./model3-outputs-gpt2tokenizer-varlength')\n",
    "## slicing the dataset to a smaller size\n",
    "\n",
    "tokenized_datasets = {\n",
    "    'train': tokenized_datasets['train'].select(np.arange(30000)),#500000\n",
    "    'validation': tokenized_datasets['validation'].select(np.arange(20000))#5000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32565,\n",
       " 13,\n",
       " 15899,\n",
       " 2497,\n",
       " 262,\n",
       " 22441,\n",
       " 1097,\n",
       " 290,\n",
       " 531,\n",
       " 11,\n",
       " 366,\n",
       " 22017,\n",
       " 11,\n",
       " 21168,\n",
       " 11,\n",
       " 534,\n",
       " 1097,\n",
       " 318,\n",
       " 523,\n",
       " 6016,\n",
       " 290,\n",
       " 3424,\n",
       " 2474,\n",
       " 21168,\n",
       " 13541,\n",
       " 290,\n",
       " 8712,\n",
       " 11,\n",
       " 366,\n",
       " 10449,\n",
       " 345,\n",
       " 11,\n",
       " 15899,\n",
       " 13,\n",
       " 314,\n",
       " 25245,\n",
       " 340,\n",
       " 790,\n",
       " 1110,\n",
       " 526,\n",
       " 198,\n",
       " 198,\n",
       " 3260,\n",
       " 2712,\n",
       " 351,\n",
       " 262,\n",
       " 1097,\n",
       " 11,\n",
       " 21168,\n",
       " 290,\n",
       " 15899,\n",
       " 2936,\n",
       " 47124,\n",
       " 13,\n",
       " 1119,\n",
       " 1043,\n",
       " 257,\n",
       " 1402,\n",
       " 16723,\n",
       " 351,\n",
       " 1598,\n",
       " 1660,\n",
       " 13,\n",
       " 1119,\n",
       " 24070,\n",
       " 262,\n",
       " 1660,\n",
       " 290,\n",
       " 2936,\n",
       " 845,\n",
       " 3772,\n",
       " 13,\n",
       " 1119,\n",
       " 2826,\n",
       " 1978,\n",
       " 477,\n",
       " 1110,\n",
       " 290,\n",
       " 2627,\n",
       " 1266,\n",
       " 2460,\n",
       " 25970,\n",
       " 82,\n",
       " 29,\n",
       " 7454,\n",
       " 2402,\n",
       " 257,\n",
       " 640,\n",
       " 11,\n",
       " 287,\n",
       " 257,\n",
       " 1263,\n",
       " 8222,\n",
       " 11,\n",
       " 612,\n",
       " 5615,\n",
       " 257,\n",
       " 9529,\n",
       " 259,\n",
       " 420,\n",
       " 27498,\n",
       " 3706,\n",
       " 371,\n",
       " 23536,\n",
       " 13,\n",
       " 371,\n",
       " 23536,\n",
       " 6151,\n",
       " 284,\n",
       " 12080,\n",
       " 13,\n",
       " 1375,\n",
       " 19952,\n",
       " 7150,\n",
       " 11,\n",
       " 12586,\n",
       " 11,\n",
       " 290,\n",
       " 18639,\n",
       " 13,\n",
       " 1881,\n",
       " 1110,\n",
       " 11,\n",
       " 371,\n",
       " 23536,\n",
       " 1043,\n",
       " 281,\n",
       " 30284,\n",
       " 12788,\n",
       " 13,\n",
       " 1375,\n",
       " 550,\n",
       " 1239,\n",
       " 1775,\n",
       " 1997,\n",
       " 588,\n",
       " 340,\n",
       " 878,\n",
       " 13,\n",
       " 632,\n",
       " 373,\n",
       " 22441,\n",
       " 290,\n",
       " 4692,\n",
       " 11,\n",
       " 290,\n",
       " 673,\n",
       " 2227,\n",
       " 284,\n",
       " 12080,\n",
       " 340,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 49,\n",
       " 23536,\n",
       " 3088,\n",
       " 284,\n",
       " 12080,\n",
       " 262,\n",
       " 30284,\n",
       " 12788,\n",
       " 11,\n",
       " 475,\n",
       " 340,\n",
       " 373,\n",
       " 845,\n",
       " 32911,\n",
       " 13,\n",
       " 1375,\n",
       " 3088,\n",
       " 757,\n",
       " 290,\n",
       " 757,\n",
       " 11,\n",
       " 475,\n",
       " 673,\n",
       " 4030,\n",
       " 7463,\n",
       " 866,\n",
       " 13,\n",
       " 371,\n",
       " 23536,\n",
       " 373,\n",
       " 6507,\n",
       " 13,\n",
       " 1375,\n",
       " 2227,\n",
       " 284,\n",
       " 12080,\n",
       " 262,\n",
       " 30284,\n",
       " 12788,\n",
       " 523,\n",
       " 881,\n",
       " 13,\n",
       " 3244,\n",
       " 11,\n",
       " 673,\n",
       " 2497,\n",
       " 257,\n",
       " 1310,\n",
       " 6512,\n",
       " 3706,\n",
       " 15890,\n",
       " 13,\n",
       " 15890,\n",
       " 2497,\n",
       " 326,\n",
       " 371,\n",
       " 23536,\n",
       " 373,\n",
       " 6507,\n",
       " 290,\n",
       " 1965,\n",
       " 11,\n",
       " 366,\n",
       " 5195,\n",
       " 389,\n",
       " 345,\n",
       " 6507,\n",
       " 11,\n",
       " 371,\n",
       " 23536,\n",
       " 1701,\n",
       " 198,\n",
       " 198,\n",
       " 49,\n",
       " 23536,\n",
       " 1297,\n",
       " 15890,\n",
       " 546,\n",
       " 262,\n",
       " 30284,\n",
       " 12788,\n",
       " 290,\n",
       " 703,\n",
       " 673,\n",
       " 3521,\n",
       " 470,\n",
       " 12080,\n",
       " 340,\n",
       " 13,\n",
       " 15890,\n",
       " 531,\n",
       " 11,\n",
       " 366,\n",
       " 40,\n",
       " 423,\n",
       " 281,\n",
       " 2126,\n",
       " 0,\n",
       " 3914,\n",
       " 338,\n",
       " 1064,\n",
       " 617,\n",
       " 1263,\n",
       " 5667,\n",
       " 284,\n",
       " 1234,\n",
       " 739,\n",
       " 534,\n",
       " 3625,\n",
       " 13,\n",
       " 1119,\n",
       " 481,\n",
       " 1037,\n",
       " 345,\n",
       " 12080,\n",
       " 262,\n",
       " 30284,\n",
       " 12788,\n",
       " 526,\n",
       " 371,\n",
       " 23536,\n",
       " 290,\n",
       " 15890,\n",
       " 3114,\n",
       " 329,\n",
       " 1263,\n",
       " 5667,\n",
       " 290,\n",
       " 1043,\n",
       " 617,\n",
       " 13,\n",
       " 371,\n",
       " 23536,\n",
       " 1234,\n",
       " 262,\n",
       " 5667,\n",
       " 739,\n",
       " 607,\n",
       " 3625,\n",
       " 290,\n",
       " 3088,\n",
       " 284,\n",
       " 12080,\n",
       " 262,\n",
       " 30284,\n",
       " 12788,\n",
       " 757,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 1212,\n",
       " 640,\n",
       " 11,\n",
       " 371,\n",
       " 23536,\n",
       " 1422,\n",
       " 470,\n",
       " 13819,\n",
       " 13,\n",
       " 1375,\n",
       " 19952,\n",
       " 290,\n",
       " 19952,\n",
       " 1566,\n",
       " 673,\n",
       " 4251,\n",
       " 262,\n",
       " 1353,\n",
       " 286,\n",
       " 262,\n",
       " 30284,\n",
       " 12788,\n",
       " 13,\n",
       " 371,\n",
       " 23536,\n",
       " 373,\n",
       " 523,\n",
       " 3772,\n",
       " 0,\n",
       " 1375,\n",
       " 290,\n",
       " 15890,\n",
       " 2826,\n",
       " 319,\n",
       " 262,\n",
       " 30284,\n",
       " 12788,\n",
       " 477,\n",
       " 1110,\n",
       " 13,\n",
       " 3574,\n",
       " 326,\n",
       " 1110,\n",
       " 319,\n",
       " 11,\n",
       " 371,\n",
       " 23536,\n",
       " 290,\n",
       " 15890,\n",
       " 547,\n",
       " 262,\n",
       " 1266,\n",
       " 286,\n",
       " 2460,\n",
       " 11,\n",
       " 290,\n",
       " 484,\n",
       " 19952,\n",
       " 290,\n",
       " 2826,\n",
       " 1978,\n",
       " 477,\n",
       " 262,\n",
       " 640,\n",
       " 13,\n",
       " 843,\n",
       " 371,\n",
       " 23536,\n",
       " 4499,\n",
       " 326,\n",
       " 351,\n",
       " 257,\n",
       " 1310,\n",
       " 1037,\n",
       " 422,\n",
       " 257,\n",
       " 1545,\n",
       " 11,\n",
       " 673,\n",
       " 714,\n",
       " 12080,\n",
       " 1997,\n",
       " 25970,\n",
       " 82,\n",
       " 29,\n",
       " 7454,\n",
       " 2402,\n",
       " 257,\n",
       " 640,\n",
       " 11,\n",
       " 287,\n",
       " 257,\n",
       " 1402,\n",
       " 12699,\n",
       " 11,\n",
       " 612,\n",
       " 373,\n",
       " 257,\n",
       " 1402,\n",
       " 12379,\n",
       " 13560,\n",
       " 13,\n",
       " 383,\n",
       " 12379,\n",
       " 13560,\n",
       " 550,\n",
       " 257,\n",
       " 1438,\n",
       " 13,\n",
       " 2332,\n",
       " 1438,\n",
       " 373,\n",
       " 40355,\n",
       " 13,\n",
       " 40355,\n",
       " 373,\n",
       " 845,\n",
       " 1402,\n",
       " 11,\n",
       " 475,\n",
       " 673,\n",
       " 373,\n",
       " 635,\n",
       " 845,\n",
       " 3772,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 3198,\n",
       " 1110,\n",
       " 11,\n",
       " 40355,\n",
       " 2497,\n",
       " 257,\n",
       " 3290,\n",
       " 13,\n",
       " 383,\n",
       " 3290,\n",
       " 373,\n",
       " 1263,\n",
       " 290,\n",
       " 550,\n",
       " 257,\n",
       " 1438,\n",
       " 1165,\n",
       " 13,\n",
       " 2399,\n",
       " 1438,\n",
       " 373,\n",
       " 5436,\n",
       " 13,\n",
       " 5436,\n",
       " 8288,\n",
       " 284,\n",
       " 711,\n",
       " 287,\n",
       " 262,\n",
       " 12699,\n",
       " 13,\n",
       " 40355,\n",
       " 8288,\n",
       " 284,\n",
       " 2342,\n",
       " 5436,\n",
       " 711,\n",
       " 13,\n",
       " 5436,\n",
       " 290,\n",
       " 40355,\n",
       " 2627,\n",
       " 2460,\n",
       " 13,\n",
       " 198,\n",
       " 198,\n",
       " 6109,\n",
       " 1110,\n",
       " 11,\n",
       " 5436,\n",
       " 561,\n",
       " 1282,\n",
       " 284,\n",
       " 262,\n",
       " 12699,\n",
       " 284,\n",
       " 711,\n",
       " 13,\n",
       " 40355,\n",
       " 561,\n",
       " 2342,\n",
       " 290,\n",
       " 8212,\n",
       " 13,\n",
       " 1119,\n",
       " 547,\n",
       " 845,\n",
       " 3772,\n",
       " 1978,\n",
       " 13,\n",
       " 843,\n",
       " 772,\n",
       " 996,\n",
       " 40355,\n",
       " 373,\n",
       " 1402,\n",
       " 11,\n",
       " 673,\n",
       " 2993,\n",
       " 326]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['validation']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input id shape is  torch.Size([1, 137])\n",
      "im here!\n",
      "i is  0  loss is  tensor(0.2377, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 261])\n",
      "im here!\n",
      "i is  1  loss is  tensor(0.3605, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 215])\n",
      "im here!\n",
      "i is  2  loss is  tensor(0.2397, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 158])\n",
      "im here!\n",
      "i is  3  loss is  tensor(0.2233, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 163])\n",
      "im here!\n",
      "i is  4  loss is  tensor(0.2123, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 129])\n",
      "im here!\n",
      "i is  5  loss is  tensor(0.2197, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 204])\n",
      "im here!\n",
      "i is  6  loss is  tensor(0.1964, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 209])\n",
      "im here!\n",
      "i is  7  loss is  tensor(0.2159, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 208])\n",
      "im here!\n",
      "i is  8  loss is  tensor(0.1898, device='cuda:0')\n",
      "input id shape is  torch.Size([1, 187])\n",
      "im here!\n",
      "i is  9  loss is  tensor(0.1670, device='cuda:0')\n",
      "average loss is  tensor(0.2262, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ave = 0\n",
    "for i in range(0,10):\n",
    "    #input_str = tokenized_datasets['train']['input_ids'][i]\n",
    "    #input_ids = tokenizer(input_str, return_tensors='pt')['input_ids']\n",
    "    input_ids = torch.tensor(tokenized_datasets['validation']['input_ids'][3500 + i*1:3500 + (i+1)*1]).to('cuda')\n",
    "    print('input id shape is ', input_ids.shape)\n",
    "    model2 = model2.to('cuda')\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        print('im here!')\n",
    "        loss_val = model2(input_ids, labels=input_ids)['loss']\n",
    "        ave += loss_val\n",
    "        print('i is ', i, ' loss is ', loss_val)\n",
    "ave = ave / 10\n",
    "print('average loss is ', ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_id length is  torch.Size([1, 173])\n",
      "tensor(3.8438)\n"
     ]
    }
   ],
   "source": [
    "## just testing the training loss is small\n",
    "input_str = tinystories['train']['text'][130]\n",
    "#input_str = ''' Once upon a time, there was a thoughtful girl named Sue. Sue loved tohelp her mom around the house. One day, her mom asked her to wipe thetable after they ate their lunch. Sue was happy to help. As Sue waswiping the table, she saw a pretty candle on the window sill. Thecandle was her mom's favorite. Sue wanted to do something nice for hermom, so she said, \"Mom, can I light the candle for you?\" Her mom said,\"Yes, but be very careful.\" Sue carefully lit the candle and put it onthe table. Her mom was so happy to see the pretty candle. They bothsat and watched the candle burn. Sue's mom said, \"Thank you, Sue, forbeing so thoughtful and careful.\" Sue felt proud that she could helpher mom. The moral of the story is to always be thoughtful and carefulwhen helping others.CLOSE TEXT IS:  Once upon a time, there was a little girl named Mia. She loved to helpher mom in the kitchen. One day, her mom asked her to unpack a bigbox. Mia was very happy to help. Inside the box, there was a newfaucet for the kitchen sink. Mia's mom said, \"This faucet is veryspecial. It can go high and low.\" Mia watched as her mom put the newfaucet on the sink. When it was high, the water came out fast. When itwas low, the water came out slow. Mia learned that helping her mom wasfun and important. She felt proud when she saw the new faucet working.The moral of the story is that helping others is a good thing to do.'''\n",
    "\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer(input_str, return_tensors='pt')['input_ids']\n",
    "    print('input_id length is ', input_ids.shape)\n",
    "    hidden_embedding = model2.encoder(input_ids).last_hidden_state.detach()[0, -1, :].unsqueeze(0)\n",
    "    decoder_hidden_inputs = model2.second_encoder(input_ids, output_hidden_states=True).hidden_states[0][0].detach()\n",
    "    updated_input = torch.cat((hidden_embedding, decoder_hidden_inputs), dim=0)\n",
    "    print(model2.decoder(inputs_embeds=updated_input, labels=torch.cat((torch.tensor([10]), input_ids[0])))['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 173])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinystories['train']['text'][10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.decoder.lm_head.weight\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
