{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mymodel definition\n",
    "from transformers import GPT2Model, GPT2LMHeadModel, GPT2Config, PreTrainedModel\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "\n",
    "class MyModel(PreTrainedModel):\n",
    "    config_class = GPT2Config\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = GPT2Model(config)\n",
    "        self.second_encoder = GPT2Model(config)\n",
    "        self.decoder = GPT2LMHeadModel(config)\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        encoder_outputs = self.encoder(input_ids)\n",
    "        hidden_embedding = encoder_outputs.last_hidden_state[:,-1,:].unsqueeze(1)\n",
    "        # just to obtain the hidden embeddings\n",
    "        with torch.no_grad():\n",
    "            decoder_hidden_inputs = self.second_encoder(input_ids, output_hidden_states=True).hidden_states[0]\n",
    "        #hidden_embedding_dim = hidden_embedding.shape[2]\n",
    "        updated_input = torch.cat((hidden_embedding, decoder_hidden_inputs), dim=1)\n",
    "        logits = self.decoder(inputs_embeds=updated_input)['logits']\n",
    "        logits = F.log_softmax(logits, dim=-1)\n",
    "        shifted_prediction_scores = logits[:, 1:-1, :]\n",
    "        \n",
    "        labels[attention_mask == 0] = -100 \n",
    "        labels = labels[:, 1:]\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        lm_loss = loss_fct(shifted_prediction_scores.contiguous().view(-1, self.config.vocab_size), labels.contiguous().view(-1))\n",
    "        return {'loss': lm_loss, 'logits':logits[:,1:,:]}\n",
    "    \n",
    "\n",
    "## defining tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-t5/t5-small')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "context_length = 700\n",
    "def tokenize(element):\n",
    "    #print('element is ', len(element['text']))\n",
    "    #return {'input_ids': []}\n",
    "    #print('len is ', ('</s>'.join(x) for x in element['text']).type)\n",
    "    outputs = tokenizer(\n",
    "        element['text'],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs['length'], outputs['input_ids']):\n",
    "        #print('last id is ', input_ids[-1])\n",
    "        if length <= context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    #print('batch length is ', len(input_batch))\n",
    "    return {'input_ids': input_batch}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MyModel were not initialized from the model checkpoint at ./model3weights_2024-07-04--16:34:15 and are newly initialized: ['decoder.lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = './model3weights_2024-07-04--16:34:15'\n",
    "model = MyModel.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't run this!!\n",
    "from datasets import load_from_disk\n",
    "tokenized_dataset = load_from_disk('model3-outputs')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "## load tinystories dataset in which every row of dataset is a single story\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset('roneneldan/TinyStories')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset_raw = ds.map(tokenize, batched=True, remove_columns=ds['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "tokenized_dataset = tokenized_dataset_raw['validation'].select(np.arange(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a thoughtful girl named Sue. Sue loved to help her mom around the house. One day, her mom asked her to wipe the table after they ate their lunch. Sue was happy to help.\\n\\nAs Sue was wiping the table, she saw a pretty candle on the window sill. The candle was her mom\\'s favorite. Sue wanted to do something nice for her mom, so she said, \"Mom, can I light the candle for you?\" Her mom said, \"Yes, but be very careful.\"\\n\\nSue carefully lit the candle and put it on the table. Her mom was so happy to see the pretty candle. They both sat and watched the candle burn. Sue\\'s mom said, \"Thank you, Sue, for being so thoughtful and careful.\" Sue felt proud that she could help her mom.\\n\\nThe moral of the story is to always be thoughtful and careful when helping others.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenized_dataset\n",
    "input_text = tokenizer.decode(tokenized_dataset['input_ids'][3])\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm is  tensor(3.1669, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## don't  run this!!\n",
    "import torch\n",
    "input_ids1 = tokenizer('Hello I am a professor!', return_tensors='pt')\n",
    "input_ids2 = tokenizer('Hi I am a professor!', return_tensors='pt')\n",
    "hidden_embedding1 = model.encoder(**input_ids1).last_hidden_state[0,-1,:]\n",
    "hidden_embedding2 = model.encoder(**input_ids2).last_hidden_state[0,-1,:]\n",
    "print('norm is ', torch.norm(hidden_embedding1 - hidden_embedding2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't run this!!\n",
    "hidden_embedding1 = model.encoder(torch.tensor([tokenized_dataset['input_ids'][0]])).last_hidden_state[0,-1,:]\n",
    "hidden_embedding1\n",
    "#type(input_ids1['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the closest neighbor\n",
    "def find_similar(input_tokens, list_of_tokens):\n",
    "    list_of_hiddens = []\n",
    "    for tokens in list_of_tokens:\n",
    "        #print('shape is ', len(tokens))\n",
    "        list_of_hiddens.append(model.encoder(torch.tensor([tokens])).last_hidden_state[0,-1,:])\n",
    "    #print('shape is ',list_of_hiddens.shape)\n",
    "    dist_min = 10000\n",
    "    close_tokens = []\n",
    "    #print('input tokens')\n",
    "    hidden_embedding = model.encoder(torch.tensor([input_tokens]))[0]#.last_hidden_state[0,-1,:]\n",
    "    count = 0\n",
    "    for hidden_embedding2 in list_of_hiddens:\n",
    "        #print('hidden_embedding2 shape is ', hidden_embedding2.shape)\n",
    "        dist = torch.norm(hidden_embedding - hidden_embedding2)\n",
    "        #print('dist is ', dist)\n",
    "        if dist < dist_min:\n",
    "            close_tokens = list_of_tokens[count]\n",
    "            dist_min = dist\n",
    "        count += 1\n",
    "        #print('hidden_embedding2 shape is ', hidden_embedding2.shape)\n",
    "    return [close_tokens, dist_min]\n",
    "    #return close_tokens\n",
    "    #return hidden_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "[close_tokens, dist_min] = find_similar(tokenized_dataset['input_ids'][3], tokenized_dataset['input_ids'][4:])\n",
    "close_text = tokenizer.decode(close_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT IS:  Once upon a time, there was a thoughtful girl named Sue. Sue loved to\n",
      "help her mom around the house. One day, her mom asked her to wipe the\n",
      "table after they ate their lunch. Sue was happy to help.  As Sue was\n",
      "wiping the table, she saw a pretty candle on the window sill. The\n",
      "candle was her mom's favorite. Sue wanted to do something nice for her\n",
      "mom, so she said, \"Mom, can I light the candle for you?\" Her mom said,\n",
      "\"Yes, but be very careful.\"  Sue carefully lit the candle and put it\n",
      "on the table. Her mom was so happy to see the pretty candle. They both\n",
      "sat and watched the candle burn. Sue's mom said, \"Thank you, Sue, for\n",
      "being so thoughtful and careful.\" Sue felt proud that she could help\n",
      "her mom.  The moral of the story is to always be thoughtful and\n",
      "careful when helping others.\n",
      "CLOSE TEXT IS:  Once upon a time, there was a kind farmer. He had a big cow. The cow\n",
      "was sad. The farmer did not know why. One day, a little boy came to\n",
      "the farm. He saw the sad cow. The boy kneeled down to talk to the cow.\n",
      "\"Why are you sad, cow?\" he asked. The cow said, \"I am lonely. I want a\n",
      "friend.\" The kind farmer heard the cow. He wanted to help. So, he got\n",
      "another cow to be friends with the sad cow. The sad cow was happy now.\n",
      "They played together every day. And the kind farmer, the little boy,\n",
      "and the two cows all lived happily ever after.</s>\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "def wrap(line):\n",
    "    broken = textwrap.wrap(line,70, break_long_words=False)\n",
    "    #print('broken is ', broken)\n",
    "    return '\\n'.join(broken)\n",
    "\n",
    "w_input_text = wrap(input_text)\n",
    "w_close_text = wrap(close_text)\n",
    "print('INPUT TEXT IS: ', w_input_text)\n",
    "print('CLOSE TEXT IS: ', w_close_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a thoughtful girl named Sue. Sue loved to\\nhelp her mom around the house. One day, her mom asked her to wipe the\\ntable after they ate their lunch. Sue was happy to help. As Sue was\\nwiping the table, she saw a pretty candle on the window sill. The\\ncandle was her mom\\'s favorite. Sue wanted to do something nice for her\\nmom, so she said, \"Mom, can I light the candle for you?\" Her mom said,\\n\"Yes, but be very careful.\" Sue carefully lit the candle and put it on\\nthe table. Her mom was so happy to see the pretty candle. They both\\nsat and watched the candle burn. Sue\\'s mom said, \"Thank you, Sue, for\\nbeing so thoughtful and careful.\" Sue felt proud that she could help\\nher mom. The moral of the story is to always be thoughtful and careful\\nwhen helping others.</s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## don't run this!!\n",
    "## look at an arbitray story\n",
    "str = wrap(tokenizer.decode(tokenized_dataset['input_ids'][3]))\n",
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat as nbf\n",
    "with open('load_model_hidden_state.ipynb', 'r') as f:\n",
    "    nb = nbf.read(f, as_version=4)\n",
    "with open('./load_model_hidden_state/two-similar-stories-found-within-700-stories.ipynb', 'w') as f:\n",
    "    nbf.write(nb, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
